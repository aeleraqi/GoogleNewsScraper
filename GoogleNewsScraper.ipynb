{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aeleraqi/GoogleNewsScraper/blob/main/GoogleNewsScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "استخراج الأخبار"
      ],
      "metadata": {
        "id": "3qCB8O2SKqta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qmgTY0ZlUH_c",
        "outputId": "51aa1e39-85c5-4f58-e10e-5a84609da499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6090 sha256=6a3c9f8e4e935250f423d112c1ad7b8d066d6d02dbb571b4d3c6e2c2d635a658\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.12 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install feedparser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "from datetime import datetime, timedelta\n",
        "import logging\n",
        "import pandas as pd\n",
        "from urllib.parse import quote\n",
        "import time\n",
        "\n",
        "# إعداد التسجيل لمتابعة العمليات\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class GoogleNewsDeepScraper:\n",
        "    def __init__(self, language='en', region='US'):\n",
        "        self.language = language\n",
        "        self.region = region\n",
        "\n",
        "    def get_month_ranges(self, start_date, end_date):\n",
        "        \"\"\"تقسيم الفترة الزمنية الكبيرة إلى فترات شهرية صغيرة\"\"\"\n",
        "        months = []\n",
        "        current = start_date\n",
        "        while current < end_date:\n",
        "            next_month = (current.replace(day=1) + timedelta(days=32)).replace(day=1)\n",
        "            if next_month > end_date:\n",
        "                next_month = end_date\n",
        "            months.append((current, next_month))\n",
        "            current = next_month\n",
        "        return months\n",
        "\n",
        "    def scrape_deep(self, query, start_date, end_date):\n",
        "        all_articles = []\n",
        "        date_ranges = self.get_month_ranges(start_date, end_date)\n",
        "\n",
        "        for s_date, e_date in date_ranges:\n",
        "            s_str = s_date.strftime('%Y-%m-%d')\n",
        "            e_str = e_date.strftime('%Y-%m-%d')\n",
        "\n",
        "            full_query = f\"{query} after:{s_str} before:{e_str}\"\n",
        "            encoded_query = quote(full_query)\n",
        "\n",
        "            rss_url = (\n",
        "                f\"https://news.google.com/rss/search?q={encoded_query}\"\n",
        "                f\"&hl={self.language}&gl={self.region}\"\n",
        "                f\"&ceid={self.region}:{self.language}\"\n",
        "            )\n",
        "\n",
        "            logging.info(f\"جاري جلب: [{query}] من {s_str} إلى {e_str}\")\n",
        "\n",
        "            try:\n",
        "                feed = feedparser.parse(rss_url)\n",
        "                for entry in feed.entries:\n",
        "                    all_articles.append({\n",
        "                        'Keyword': query,\n",
        "                        'Title': entry.title,\n",
        "                        'Link': entry.link,\n",
        "                        'Published': entry.published,\n",
        "                        'Source': entry.source.title if hasattr(entry, 'source') else 'Unknown',\n",
        "                        'Language': self.language,\n",
        "                        'Region': self.region\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                logging.error(f\"خطأ في الجلب: {e}\")\n",
        "\n",
        "            time.sleep(0.6)\n",
        "\n",
        "        return all_articles\n",
        "\n",
        "\n",
        "def run_news_scraper():\n",
        "    print(\"\\n--- إعدادات البحث العميق في أخبار جوجل ---\")\n",
        "\n",
        "    keywords_input = input(\"أدخل الكلمات المفتاحية (افصل بينها بفاصلة): \")\n",
        "    keywords = [k.strip() for k in keywords_input.split(',')]\n",
        "\n",
        "    language = input(\"أدخل لغة الأخبار (ar / en / fr): \").strip().lower()\n",
        "    region = input(\"أدخل الدولة/المنطقة (EG / US / GB): \").strip().upper()\n",
        "\n",
        "    start_str = input(\"تاريخ البداية (YYYY-MM-DD): \")\n",
        "    end_str = input(\"تاريخ النهاية (YYYY-MM-DD): \")\n",
        "\n",
        "    try:\n",
        "        start_dt = datetime.strptime(start_str, '%Y-%m-%d')\n",
        "        end_dt = datetime.strptime(end_str, '%Y-%m-%d')\n",
        "    except ValueError:\n",
        "        print(\"❌ خطأ في تنسيق التاريخ\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    scraper = GoogleNewsDeepScraper(language=language, region=region)\n",
        "    final_list = []\n",
        "\n",
        "    for kw in keywords:\n",
        "        results = scraper.scrape_deep(kw, start_dt, end_dt)\n",
        "        final_list.extend(results)\n",
        "        print(f\"✅ تم جمع {len(results)} خبر مبدئي للكلمة: {kw}\")\n",
        "\n",
        "    if not final_list:\n",
        "        print(\"❌ لم يتم العثور على بيانات\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.DataFrame(final_list)\n",
        "\n",
        "    total_before = len(df)\n",
        "    df = df.drop_duplicates(subset=['Title'])\n",
        "    total_after = len(df)\n",
        "\n",
        "    filename = f\"news_archive_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"إجمالي الأخبار المجمعة: {total_before}\")\n",
        "    print(f\"عدد الأخبار الفريدة: {total_after}\")\n",
        "    print(f\"تم حذف {total_before - total_after} خبر مكرر\")\n",
        "    print(f\"تم حفظ الملف: {filename}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# تشغيل الكود\n",
        "df = run_news_scraper()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhXUPqHNEYDQ",
        "outputId": "1a94ee7f-7b8a-4840-a698-92149f734d2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- إعدادات البحث العميق في أخبار جوجل ---\n",
            "أدخل الكلمات المفتاحية (افصل بينها بفاصلة): donald trump\n",
            "أدخل لغة الأخبار (ar / en / fr): en\n",
            "أدخل الدولة/المنطقة (EG / US / GB): US\n",
            "تاريخ البداية (YYYY-MM-DD): 2025-01-01\n",
            "تاريخ النهاية (YYYY-MM-DD): 2025-12-31\n",
            "✅ تم جمع 1200 خبر مبدئي للكلمة: donald trump\n",
            "\n",
            "========================================\n",
            "إجمالي الأخبار المجمعة: 1200\n",
            "عدد الأخبار الفريدة: 1177\n",
            "تم حذف 23 خبر مكرر\n",
            "تم حفظ الملف: news_archive_20260204_092914.csv\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/news_archive_20260204_092914.csv\")"
      ],
      "metadata": {
        "id": "L1BH7ldLJma_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. إحصائيات الكلمات المفتاحية (Keyword)\n",
        "print(\"--- توزيع المقالات حسب الكلمة المفتاحية ---\")\n",
        "print(df['Keyword'].value_counts())\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 2. إحصائيات المصادر (Source) - أعلى 10 مصادر\n",
        "print(\"\\n--- أعلى 10 مصادر إخبارية تكراراً ---\")\n",
        "print(df['Source'].value_counts().head(10))\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 3. تحليل تاريخ النشر (Published)\n",
        "# أولاً: تحويل العمود إلى صيغة تاريخ حقيقية لأنها تأتي كنص من RSS\n",
        "df['Published_DT'] = pd.to_datetime(df['Published'], errors='coerce', utc=True)\n",
        "\n",
        "# ثانياً: استخراج السنة والشهر للتحليل الزمني\n",
        "df['Year_Month'] = df['Published_DT'].dt.to_period('M')\n",
        "\n",
        "print(\"\\n--- توزيع الأخبار حسب الشهر (Timeline) ---\")\n",
        "print(df['Year_Month'].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxUYAgzpJ5vO",
        "outputId": "827e8ed9-f6ac-4d75-8c49-7c70bb86a0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- توزيع المقالات حسب الكلمة المفتاحية ---\n",
            "Keyword\n",
            "Egypt politics               1008\n",
            "Egypt economy                 982\n",
            "Egypt security                962\n",
            "Egyptian presidency           774\n",
            "Egyptian military             745\n",
            "human rights Egypt            503\n",
            "Egyptian parliament           469\n",
            "Egyptian pound                318\n",
            "Sisi government               315\n",
            "counterterrorism Egypt        308\n",
            "inflation Egypt               286\n",
            "political prisoners Egypt     283\n",
            "press freedom Egypt           277\n",
            "IMF Egypt                     254\n",
            "Sinai insurgency               13\n",
            "Name: count, dtype: int64\n",
            "------------------------------\n",
            "\n",
            "--- أعلى 10 مصادر إخبارية تكراراً ---\n",
            "Source\n",
            "Ahram Online           604\n",
            "Dailynewsegypt         561\n",
            "Egypt Today            338\n",
            "Egypt Independent      175\n",
            "Middle East Monitor    147\n",
            "The New Arab           126\n",
            "Reuters                121\n",
            "The Jerusalem Post     111\n",
            "Arab News              109\n",
            "Middle East Eye        100\n",
            "Name: count, dtype: int64\n",
            "------------------------------\n",
            "\n",
            "--- توزيع الأخبار حسب الشهر (Timeline) ---\n",
            "Year_Month\n",
            "2025-01    568\n",
            "2025-02    609\n",
            "2025-03    530\n",
            "2025-04    574\n",
            "2025-05    569\n",
            "2025-06    606\n",
            "2025-07    607\n",
            "2025-08    598\n",
            "2025-09    694\n",
            "2025-10    755\n",
            "2025-11    737\n",
            "2025-12    650\n",
            "Freq: M, Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1597043222.py:16: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
            "  df['Year_Month'] = df['Published_DT'].dt.to_period('M')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q76Z9Mmy6N3a",
        "outputId": "79888093-4b3d-42b3-a334-6649a8d53649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Keyword                                              Title  \\\n",
              "0           Egypt politics  Egypt: Repression intensifies ahead of human r...   \n",
              "1           Egypt politics  Egypt Remains Cool to the New Syrian Governmen...   \n",
              "2           Egypt politics  World Report 2025: Rights Trends in Egypt - Hu...   \n",
              "3           Egypt politics  Assad’s Fall Sparks Fear and Reflection in Egy...   \n",
              "4           Egypt politics  Reconsidering Cairo's Approach to the Sudanese...   \n",
              "...                    ...                                                ...   \n",
              "10038  press freedom Egypt  Salah to the rescue after Egypt start slow - P...   \n",
              "10039  press freedom Egypt  Israel approves closing of military radio - Wa...   \n",
              "10040  press freedom Egypt  Nigeria's AFCON 2025 Journey Begins: Super Eag...   \n",
              "10041  press freedom Egypt  Salah focused on AFCON despite Liverpool uncer...   \n",
              "10042  press freedom Egypt  Nigeria down Table Tennis powerhouse Egypt, to...   \n",
              "\n",
              "                                                    Link  \\\n",
              "0      https://news.google.com/rss/articles/CBMisgFBV...   \n",
              "1      https://news.google.com/rss/articles/CBMiigFBV...   \n",
              "2      https://news.google.com/rss/articles/CBMibEFVX...   \n",
              "3      https://news.google.com/rss/articles/CBMiZkFVX...   \n",
              "4      https://news.google.com/rss/articles/CBMiogFBV...   \n",
              "...                                                  ...   \n",
              "10038  https://news.google.com/rss/articles/CBMid0FVX...   \n",
              "10039  https://news.google.com/rss/articles/CBMirwFBV...   \n",
              "10040  https://news.google.com/rss/articles/CBMimAFBV...   \n",
              "10041  https://news.google.com/rss/articles/CBMilgFBV...   \n",
              "10042  https://news.google.com/rss/articles/CBMinwFBV...   \n",
              "\n",
              "                           Published                     Source Language  \\\n",
              "0      Mon, 27 Jan 2025 08:00:00 GMT      Amnesty International       en   \n",
              "1      Thu, 23 Jan 2025 08:00:00 GMT  Arab Center Washington DC       en   \n",
              "2      Thu, 16 Jan 2025 14:05:39 GMT         Human Rights Watch       en   \n",
              "3      Tue, 07 Jan 2025 08:00:00 GMT         New Lines Magazine       en   \n",
              "4      Thu, 30 Jan 2025 08:00:00 GMT   The Washington Institute       en   \n",
              "...                              ...                        ...      ...   \n",
              "10038  Tue, 23 Dec 2025 06:45:13 GMT                PressReader       en   \n",
              "10039  Mon, 22 Dec 2025 13:55:04 GMT               Ahram Online       en   \n",
              "10040  Tue, 23 Dec 2025 15:15:37 GMT                Head Topics       en   \n",
              "10041  Sun, 21 Dec 2025 21:09:11 GMT       Blueprint Newspapers       en   \n",
              "10042  Tue, 16 Dec 2025 11:51:53 GMT       Blueprint Newspapers       en   \n",
              "\n",
              "      Region              Published_DT Year_Month  \n",
              "0         US 2025-01-27 08:00:00+00:00    2025-01  \n",
              "1         US 2025-01-23 08:00:00+00:00    2025-01  \n",
              "2         US 2025-01-16 14:05:39+00:00    2025-01  \n",
              "3         US 2025-01-07 08:00:00+00:00    2025-01  \n",
              "4         US 2025-01-30 08:00:00+00:00    2025-01  \n",
              "...      ...                       ...        ...  \n",
              "10038     US 2025-12-23 06:45:13+00:00    2025-12  \n",
              "10039     US 2025-12-22 13:55:04+00:00    2025-12  \n",
              "10040     US 2025-12-23 15:15:37+00:00    2025-12  \n",
              "10041     US 2025-12-21 21:09:11+00:00    2025-12  \n",
              "10042     US 2025-12-16 11:51:53+00:00    2025-12  \n",
              "\n",
              "[7497 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e094446c-8818-4c77-85a9-0ef15189ca58\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Keyword</th>\n",
              "      <th>Title</th>\n",
              "      <th>Link</th>\n",
              "      <th>Published</th>\n",
              "      <th>Source</th>\n",
              "      <th>Language</th>\n",
              "      <th>Region</th>\n",
              "      <th>Published_DT</th>\n",
              "      <th>Year_Month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Egypt politics</td>\n",
              "      <td>Egypt: Repression intensifies ahead of human r...</td>\n",
              "      <td>https://news.google.com/rss/articles/CBMisgFBV...</td>\n",
              "      <td>Mon, 27 Jan 2025 08:00:00 GMT</td>\n",
              "      <td>Amnesty International</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2025-01-27 08:00:00+00:00</td>\n",
              "      <td>2025-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Egypt politics</td>\n",
              "      <td>Egypt Remains Cool to the New Syrian Governmen...</td>\n",
              "      <td>https://news.google.com/rss/articles/CBMiigFBV...</td>\n",
              "      <td>Thu, 23 Jan 2025 08:00:00 GMT</td>\n",
              "      <td>Arab Center Washington DC</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2025-01-23 08:00:00+00:00</td>\n",
              "      <td>2025-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Egypt politics</td>\n",
              "      <td>World Report 2025: Rights Trends in Egypt - Hu...</td>\n",
              "      <td>https://news.google.com/rss/articles/CBMibEFVX...</td>\n",
              "      <td>Thu, 16 Jan 2025 14:05:39 GMT</td>\n",
              "      <td>Human Rights Watch</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2025-01-16 14:05:39+00:00</td>\n",
              "      <td>2025-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Egypt politics</td>\n",
              "      <td>Assad’s Fall Sparks Fear and Reflection in Egy...</td>\n",
              "      <td>https://news.google.com/rss/articles/CBMiZkFVX...</td>\n",
              "      <td>Tue, 07 Jan 2025 08:00:00 GMT</td>\n",
              "      <td>New Lines Magazine</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2025-01-07 08:00:00+00:00</td>\n",
              "      <td>2025-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Egypt politics</td>\n",
              "      <td>Reconsidering Cairo's Approach to the Sudanese...</td>\n",
              "      <td>https://news.google.com/rss/articles/CBMiogFBV...</td>\n",
              "      <td>Thu, 30 Jan 2025 08:00:00 GMT</td>\n",
              "      <td>The Washington Institute</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2025-01-30 08:00:00+00:00</td>\n",
              "      <td>2025-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10038</th>\n",
              "      <td>press freedom Egypt</td>\n",
              "      <td>Salah to the rescue after Egypt start slow - P...</td>\n",
              "      <td>https://news.google.com/rss/articles/CBMid0FVX...</td>\n",
              "      <td>Tue, 23 Dec 2025 06:45:13 GMT</td>\n",
              "      <td>PressReader</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2025-12-23 06:45:13+00:00</td>\n",
              "      <td>2025-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10039</th>\n",
              "      <td>press freedom Egypt</td>\n",
              "      <td>Israel approves closing of military radio - Wa...</td>\n",
              "      <td>https://news.google.com/rss/articles/CBMirwFBV...</td>\n",
              "      <td>Mon, 22 Dec 2025 13:55:04 GMT</td>\n",
              "      <td>Ahram Online</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2025-12-22 13:55:04+00:00</td>\n",
              "      <td>2025-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10040</th>\n",
              "      <td>press freedom Egypt</td>\n",
              "      <td>Nigeria's AFCON 2025 Journey Begins: Super Eag...</td>\n",
              "      <td>https://news.google.com/rss/articles/CBMimAFBV...</td>\n",
              "      <td>Tue, 23 Dec 2025 15:15:37 GMT</td>\n",
              "      <td>Head Topics</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2025-12-23 15:15:37+00:00</td>\n",
              "      <td>2025-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10041</th>\n",
              "      <td>press freedom Egypt</td>\n",
              "      <td>Salah focused on AFCON despite Liverpool uncer...</td>\n",
              "      <td>https://news.google.com/rss/articles/CBMilgFBV...</td>\n",
              "      <td>Sun, 21 Dec 2025 21:09:11 GMT</td>\n",
              "      <td>Blueprint Newspapers</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2025-12-21 21:09:11+00:00</td>\n",
              "      <td>2025-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10042</th>\n",
              "      <td>press freedom Egypt</td>\n",
              "      <td>Nigeria down Table Tennis powerhouse Egypt, to...</td>\n",
              "      <td>https://news.google.com/rss/articles/CBMinwFBV...</td>\n",
              "      <td>Tue, 16 Dec 2025 11:51:53 GMT</td>\n",
              "      <td>Blueprint Newspapers</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2025-12-16 11:51:53+00:00</td>\n",
              "      <td>2025-12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7497 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e094446c-8818-4c77-85a9-0ef15189ca58')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e094446c-8818-4c77-85a9-0ef15189ca58 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e094446c-8818-4c77-85a9-0ef15189ca58');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1ba60dfc-141f-44ec-849a-166328e07de1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1ba60dfc-141f-44ec-849a-166328e07de1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1ba60dfc-141f-44ec-849a-166328e07de1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_9346241a-cd17-4f1b-a3df-98ea2a1702aa\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9346241a-cd17-4f1b-a3df-98ea2a1702aa button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7497,\n  \"fields\": [\n    {\n      \"column\": \"Keyword\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Egyptian pound\",\n          \"IMF Egypt\",\n          \"Egypt politics\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7497,\n        \"samples\": [\n          \"International Monetary Fund to Combine Egypt\\u2019s 5th & 6th Reviews - cairoscene.com\",\n          \"Singapore\\u2019s Destiny Energy to invest $210m in Egypt to produce 100,000 tonnes of green ammonia annually - Dailynewsegypt\",\n          \"EU lists seven \\u2018safe\\u2019 countries as means to facilitate deportation - France 24\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Link\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7497,\n        \"samples\": [\n          \"https://news.google.com/rss/articles/CBMilAFBVV95cUxQVWlCQXlLRzBQUlhmbjZSY0FLZVFWZkZYMWpETG05cU9Ub0JJUm1La0NwYnY4R3ZLS2tpNldSNXNia2RfMVJBVGU4bFZVaGxnbGRONG1yNVhyMXhMengzQkZDUFRwVEFxay1jSFRlMUplNjltR1JkbnZ2Z2g2cXlpblROMmx2UU9Xbks1SVN2cHZOZzdu?oc=5\",\n          \"https://news.google.com/rss/articles/CBMi2wFBVV95cUxPVVNaNHJQWGZ3NWlNMEFDMV9EWmVON1J1OGtkdHpWeklKUXc3ZGd2Z1g4X3hrVXp5TjI5YjN5c3p6VmtZWHo4OVhEM2VMMGtRR0NGVVVzbTh2M3VveDN1dVRDVE5zVlNSZXhsZEx2QjVsS3FEdEdMRnRVMGNxYWE2a2wyUHJlaWZWQjUtSlZWNV9jRE1PeE9Ya1puU1diZlQtWEpVeERhYkFpR19OUjRkdVB1ZjlWTVRfQUxJSHFBWk5YWXBOTDU4bHFIUmJvWHoxNzhZdl9nTGZET1U?oc=5\",\n          \"https://news.google.com/rss/articles/CBMitgFBVV95cUxNNHhzZUpaTzJ2bnI0X0R6bTR3WkNWWXk2REJnOHAycE1IU1dPdnhmX0R1NXNOT3drNVNvZWtDSEVIMWF1dVg1Y2xFNHNsbElnNWRoZkVCbllUVzNJeVRpczM0YzI4WWJxNHptVklaYWlJSll5YnlQOGtVTlh6OUdhU2U2OWlsam45RDJOeGVzRHZXb3FKeGNFX282UHhOMGNjR2REZkZHVjg0VUNjcEZqdWhaWVNXZw?oc=5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Published\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 911,\n        \"samples\": [\n          \"Tue, 23 Dec 2025 10:19:00 GMT\",\n          \"Fri, 19 Sep 2025 09:55:24 GMT\",\n          \"Fri, 28 Nov 2025 09:03:23 GMT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1219,\n        \"samples\": [\n          \"Anash.org\",\n          \"Fuel Cells Works\",\n          \"Australian Broadcasting Corporation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"en\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"US\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Published_DT\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-01-01 08:00:00+00:00\",\n        \"max\": \"2025-12-25 07:52:15+00:00\",\n        \"num_unique_values\": 911,\n        \"samples\": [\n          \"2025-12-23 10:19:00+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year_Month\",\n      \"properties\": {\n        \"dtype\": \"period[M]\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"2025-11\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "استخراج المحتوى"
      ],
      "metadata": {
        "id": "MNAGe3qaKl8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pcsdpNiiBN-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157d0104-6d80-462e-974d-bbca362e7fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (4.13.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (12.0.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.3)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting lxml>=3.6.0 (from newspaper3k)\n",
            "  Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.32.4)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2025.11.12)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-3.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.20.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m139.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-3.0.1-py2.py3-none-any.whl (4.5 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13634 sha256=501a44abc666c6d79ee0bf866af6527090fdc24171c2d0ff70ebda872b6e741b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/91/9f/00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n",
            "  Building wheel for feedfinder2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3393 sha256=29c289639311ef30f552ad9241187a1a3c80cdeb9ee1b662a16108f2435bba6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/9f/fb/364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n",
            "  Building wheel for jieba3k (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398402 sha256=47b0032a1349d17ade43ab23dddde0844298f6420610b64be908a04985a9ee50\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/72/f7/fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n",
            "  Building wheel for sgmllib3k (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=56fc68d1e31fcddfed4692cb74f555a34989297c8f328befd413dfc824efca7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, lxml, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.12 jieba3k-0.35.1 lxml-6.0.2 newspaper3k-0.2.8 requests-file-3.0.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A_DgKqeFK6RD",
        "outputId": "dfcc5fc7-a5ea-49b3-bdec-0b9c057fbcd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openpyxl\n",
            "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting et-xmlfile (from openpyxl)\n",
            "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/250.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml_html_clean"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yFlslbEcH2xK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d892e142-7afd-46bb-8cbe-c1f93f146102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from lxml_html_clean) (6.0.2)\n",
            "Downloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from newspaper import Article\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# 1. دالة سحب محسنة مع معالجة أفضل للأخطاء\n",
        "def scrape_article(url):\n",
        "    if pd.isna(url) or str(url).strip() == \"\":\n",
        "        return {\"Content\": None, \"Parsed_Date\": None, \"Error\": \"URL فارغ\"}\n",
        "    try:\n",
        "        article = Article(url, language='ar')\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        text = article.text.strip()\n",
        "        return {\n",
        "            \"Content\": text if text else None,\n",
        "            \"Parsed_Date\": article.publish_date\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"Content\": None, \"Parsed_Date\": None, \"Error\": str(e)}\n",
        "\n",
        "# 2. البدء في المعالجة\n",
        "if 'df' in locals() and not df.empty:\n",
        "    # تأكد من أننا نستخدم عمود 'Link' الصحيح\n",
        "    links = df['Link'].tolist()\n",
        "    print(f\"🚀 بدء معالجة {len(links)} رابط فعلياً...\")\n",
        "\n",
        "    texts, publish_dates, errors = [], [], []\n",
        "\n",
        "    # سنقوم بمعالجة أول 10 روابط فقط كاختبار سريع للتأكد من أن السحب يعمل\n",
        "    # إذا كنت متأكداً، استبدل links[:10] بـ links لمعالجة الكل\n",
        "    for url in tqdm(links, desc=\"جاري سحب المحتوى\"):\n",
        "        result = scrape_article(url)\n",
        "        texts.append(result.get(\"Content\"))\n",
        "        publish_dates.append(result.get(\"Parsed_Date\"))\n",
        "        errors.append(result.get(\"Error\"))\n",
        "        # إضافة تأخير بسيط جداً لمنع الحظر\n",
        "        # time.sleep(0.1)\n",
        "\n",
        "    df['Full_Text'] = texts\n",
        "    df['Article_Publish_Date'] = publish_dates\n",
        "    df['Scrape_Error'] = errors\n",
        "\n",
        "    # 3. تنظيف قاطع لكل أعمدة التاريخ من الـ Timezone\n",
        "    print(\"🧹 تنظيف شامل لكل تواريخ الـ DataFrame...\")\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[col]) or df[col].dtype == 'object':\n",
        "            try:\n",
        "                df[col] = pd.to_datetime(df[col], errors='ignore')\n",
        "                if hasattr(df[col], 'dt'):\n",
        "                    df[col] = df[col].dt.tz_localize(None)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # 4. الحفظ النهائي\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    filename = f'final_data_{timestamp}.xlsx'\n",
        "\n",
        "    try:\n",
        "        # إجبار الحفظ وتجاوز أي خطأ في التنسيق بجعل التواريخ نصوصاً إذا لزم الأمر\n",
        "        df.to_excel(filename, index=False, engine='openpyxl')\n",
        "        print(f\"✅ تم حفظ الملف بنجاح: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ فشل الإكسل، جاري حفظ CSV: {e}\")\n",
        "        df.to_csv(filename.replace('.xlsx', '.csv'), index=False, encoding='utf-8-sig')\n",
        "\n",
        "else:\n",
        "    print(\"❌ لا يوجد بيانات في df\")\n",
        "\n",
        "# فحص عدد المقالات التي نجحنا في سحب نصها فعلياً\n",
        "successful_scrapes = df['Full_Text'].notna().sum()\n",
        "print(f\"📊 إحصائية: تم سحب محتوى {successful_scrapes} مقال من أصل {len(df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuvWP-EUKkic",
        "outputId": "2eb663d6-f4de-434c-c61d-5af9122266f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 بدء معالجة 17900 رابط فعلياً...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "جاري سحب المحتوى:  49%|████▉     | 8774/17900 [11:31:37<16:34:05,  6.54s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def save_and_download(df):\n",
        "    # 1. تنظيف أخير وشامل لكل أعمدة التاريخ لحل مشكلة الإكسل (Timezone)\n",
        "    print(\"🧹 جاري تجهيز التواريخ للتوافق مع الإكسل...\")\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[col]) or df[col].dtype == 'object':\n",
        "            try:\n",
        "                # تحويل إلى تاريخ بدون منطقة زمنية\n",
        "                df[col] = pd.to_datetime(df[col], errors='ignore')\n",
        "                if hasattr(df[col], 'dt'):\n",
        "                    df[col] = df[col].dt.tz_localize(None)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    # 2. إنشاء اسم ملف فريد يحتوي على الوقت الحالي\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
        "    excel_name = f'News_Data_Final_{timestamp}.xlsx'\n",
        "    csv_name = f'News_Data_Final_{timestamp}.csv'\n",
        "\n",
        "    try:\n",
        "        # 3. محاولة حفظ الملف بصيغة Excel\n",
        "        df.to_excel(excel_name, index=False, engine='openpyxl')\n",
        "        print(f\"✅ تم حفظ الملف في ذاكرة السحابة باسم: {excel_name}\")\n",
        "        final_file = excel_name\n",
        "    except Exception as e:\n",
        "        # 4. حل احتياطي: حفظ بصيغة CSV إذا فشل الإكسل\n",
        "        print(f\"⚠️ فشل حفظ Excel بسبب: {e}. سيتم الحفظ بصيغة CSV.\")\n",
        "        df.to_csv(csv_name, index=False, encoding='utf-8-sig')\n",
        "        final_file = csv_name\n",
        "\n",
        "    # 5. كود إضافي لمستخدمي Google Colab لتحميل الملف لجهازك فوراً\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(f\"📥 جاري بدء تحميل الملف لجهازك الشخصي...\")\n",
        "        files.download(final_file)\n",
        "    except ImportError:\n",
        "        # إذا كنت تعمل على جهازك الشخصي (Local VS Code / PyCharm)\n",
        "        full_path = os.path.abspath(final_file)\n",
        "        print(f\"📂 الملف جاهز الآن في هذا المسار على جهازك:\\n{full_path}\")\n",
        "\n",
        "# تشغيل الدالة\n",
        "save_and_download(df)"
      ],
      "metadata": {
        "id": "yNbZFMPedjOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4NixLGnsm05"
      },
      "source": [
        "**Visuals**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGZd3NDhruKS"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Line chart for articles over time\n",
        "line_chart = go.Figure()\n",
        "\n",
        "line_chart.add_trace(go.Scatter(\n",
        "    x=articles_per_day.index,\n",
        "    y=articles_per_day.values,\n",
        "    mode='lines+markers',\n",
        "    name='Articles'\n",
        "))\n",
        "\n",
        "line_chart.update_layout(\n",
        "    title='Articles Over Time',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Number of Articles',\n",
        "    xaxis=dict(type='category', tickangle=45)\n",
        ")\n",
        "line_chart.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HtzeAMVsmTz"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Reset index to include the index as a column\n",
        "articles_per_source_df = articles_per_source.reset_index()\n",
        "articles_per_source_df.columns = ['Source', 'Count']\n",
        "\n",
        "# Bar chart\n",
        "bar_chart = px.bar(\n",
        "    articles_per_source_df,\n",
        "    x='Source',\n",
        "    y='Count',\n",
        "    title='Articles Per Source',\n",
        "    labels={'Source': 'Source', 'Count': 'Number of Articles'}\n",
        ")\n",
        "bar_chart.update_layout(xaxis_title=\"Source\", yaxis_title=\"Number of Articles\")\n",
        "bar_chart.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaO5JYE1uUA1"
      },
      "outputs": [],
      "source": [
        "# Donut chart for article distribution by source\n",
        "donut_chart = px.pie(\n",
        "    articles_per_source_df,\n",
        "    names='Source',\n",
        "    values='Count',\n",
        "    title='Article Distribution by Source',\n",
        "    hole=0.4,\n",
        "    labels={'Source': 'Source', 'Count': 'Number of Articles'}\n",
        ")\n",
        "donut_chart.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuxF1FQr2dZX"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to clean and stem text\n",
        "def clean_and_stem_text(text, sources):\n",
        "    # Remove special characters\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Remove source names\n",
        "    for source in sources:\n",
        "        text = re.sub(rf\"\\b{re.escape(source)}\\b\", \"\", text, flags=re.IGNORECASE)\n",
        "    # Remove stopwords and apply stemming\n",
        "    text = \" \".join([stemmer.stem(word) for word in text.split() if word.lower() not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply cleaning and stemming function to 'Title' column\n",
        "df['Stemmed_Title'] = df['Title'].apply(lambda x: clean_and_stem_text(x, source_names))\n",
        "\n",
        "# Extract single words (unigrams) using CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')  # Unigrams by default\n",
        "X = vectorizer.fit_transform(df['Stemmed_Title'])\n",
        "unigram_matrix = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Aggregate unigram frequencies\n",
        "unigram_frequencies = unigram_matrix.sum(axis=0).sort_values(ascending=False).head(10)\n",
        "\n",
        "# Convert unigram frequencies into a DataFrame for bar chart\n",
        "unigram_df = unigram_frequencies.reset_index()\n",
        "unigram_df.columns = ['Word', 'Frequency']\n",
        "\n",
        "# Bar chart for unigram frequencies\n",
        "unigram_bar_chart = px.bar(\n",
        "    unigram_df,\n",
        "    x='Word',\n",
        "    y='Frequency',\n",
        "    title=\"Top 10 Stemmed Words in Titles (Source Names Removed)\",\n",
        "    labels={'Word': 'Word', 'Frequency': 'Frequency'},\n",
        "    text='Frequency'\n",
        ")\n",
        "\n",
        "# Customize layout for better readability\n",
        "unigram_bar_chart.update_layout(\n",
        "    xaxis_title=\"Word\",\n",
        "    yaxis_title=\"Frequency\",\n",
        "    xaxis=dict(tickangle=45),  # Rotate x-axis labels for better readability\n",
        "    title_font_size=20,\n",
        "    font=dict(size=14),\n",
        "    bargap=0.2  # Space between bars\n",
        ")\n",
        "\n",
        "# Add text labels to bars\n",
        "unigram_bar_chart.update_traces(\n",
        "    textposition='outside',\n",
        "    marker_color='skyblue',\n",
        "    marker_line_color='blue',\n",
        "    marker_line_width=1.5\n",
        ")\n",
        "\n",
        "unigram_bar_chart.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QnKoXgwvo6U"
      },
      "outputs": [],
      "source": [
        "# Convert bigram frequencies into a DataFrame for bar chart\n",
        "bigram_df = bigram_frequencies.reset_index()\n",
        "bigram_df.columns = ['Bigram', 'Frequency']\n",
        "\n",
        "# Bar chart for bigram frequencies\n",
        "bigram_bar_chart = px.bar(\n",
        "    bigram_df,\n",
        "    x='Bigram',\n",
        "    y='Frequency',\n",
        "    title=\"Top 10 Bigrams in Titles (Source Names Removed)\",\n",
        "    labels={'Bigram': 'Bigram', 'Frequency': 'Frequency'},\n",
        "    text='Frequency'\n",
        ")\n",
        "\n",
        "# Customize layout for better readability\n",
        "bigram_bar_chart.update_layout(\n",
        "    xaxis_title=\"Bigram\",\n",
        "    yaxis_title=\"Frequency\",\n",
        "    xaxis=dict(tickangle=45),  # Rotate x-axis labels for better readability\n",
        "    title_font_size=20,\n",
        "    font=dict(size=14),\n",
        "    bargap=0.2  # Space between bars\n",
        ")\n",
        "\n",
        "# Add text labels to bars\n",
        "bigram_bar_chart.update_traces(\n",
        "    textposition='outside',\n",
        "    marker_color='skyblue',\n",
        "    marker_line_color='blue',\n",
        "    marker_line_width=1.5\n",
        ")\n",
        "\n",
        "bigram_bar_chart.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO3mXyXexCjd"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "\n",
        "# Download NLTK stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize, remove stopwords, and count keyword occurrences\n",
        "keywords = [[word for word in title.split() if word.lower() not in stop_words] for title in df['Title']]\n",
        "all_keywords = [word for words in keywords for word in words]\n",
        "keyword_counts = Counter(all_keywords)\n",
        "\n",
        "# Filter keywords with minimum frequency\n",
        "min_frequency = 2\n",
        "filtered_keywords = {word for word, count in keyword_counts.items() if count >= min_frequency}\n",
        "\n",
        "# Build filtered keyword pairs\n",
        "keyword_pairs = [\n",
        "    (word1, word2)\n",
        "    for words in keywords\n",
        "    for i, word1 in enumerate(words)\n",
        "    for word2 in words[i + 1:]\n",
        "    if word1 in filtered_keywords and word2 in filtered_keywords\n",
        "]\n",
        "\n",
        "# Create graph\n",
        "G = nx.Graph()\n",
        "G.add_edges_from(keyword_pairs)\n",
        "\n",
        "# Remove isolated nodes\n",
        "G.remove_nodes_from(list(nx.isolates(G)))\n",
        "\n",
        "# Apply community detection (clustering)\n",
        "communities = greedy_modularity_communities(G)\n",
        "\n",
        "# Assign cluster labels to nodes\n",
        "node_colors = {}\n",
        "for idx, community in enumerate(communities):\n",
        "    for node in community:\n",
        "        node_colors[node] = idx\n",
        "\n",
        "# Map node colors\n",
        "colors = [node_colors[node] for node in G.nodes]\n",
        "\n",
        "# Draw graph with clusters\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G, k=0.3, iterations=50)\n",
        "nx.draw(\n",
        "    G,\n",
        "    pos,\n",
        "    with_labels=True,\n",
        "    node_size=500,\n",
        "    node_color=colors,\n",
        "    cmap=plt.cm.tab20,\n",
        "    font_size=10,\n",
        "    edge_color=\"gray\"\n",
        ")\n",
        "plt.title('Clustered Keyword Co-occurrence Network')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldTn3q_IRt9T"
      },
      "outputs": [],
      "source": [
        "import feedparser\n",
        "from datetime import datetime, timedelta\n",
        "import logging\n",
        "import pandas as pd\n",
        "from urllib.parse import quote\n",
        "import time  # For adding delays\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Define the Google News Feed Scraper class\n",
        "class GoogleNewsFeedScraper:\n",
        "    def __init__(self, query, start_date, end_date, language, region=\"US\"):\n",
        "        self.query = query\n",
        "        self.start_date = start_date  # Already a datetime object\n",
        "        self.end_date = end_date      # Already a datetime object\n",
        "        self.language = language\n",
        "        self.region = region\n",
        "\n",
        "    def scrape_google_news_feed(self):\n",
        "        articles = []\n",
        "        current_date = self.start_date\n",
        "\n",
        "        while current_date <= self.end_date:\n",
        "            # Create a time-bound filter by adding a range to the query\n",
        "            date_filter = f\"after:{current_date.strftime('%Y-%m-%d')} before:{(current_date + timedelta(days=1)).strftime('%Y-%m-%d')}\"\n",
        "            query_with_date = f\"{self.query} {date_filter}\"\n",
        "\n",
        "            # Encode the query and construct the RSS URL\n",
        "            encoded_query = quote(query_with_date)\n",
        "            rss_url = f'https://news.google.com/rss/search?q={encoded_query}&hl={self.language}&gl={self.region}&ceid={self.region}:{self.language[:2]}'\n",
        "\n",
        "            # Log the constructed RSS URL\n",
        "            logging.info(f\"Fetching RSS feed: {rss_url}\")\n",
        "\n",
        "            # Parse the feed with a delay to avoid rate-limiting issues\n",
        "            time.sleep(1)  # Add a delay between requests\n",
        "            feed = feedparser.parse(rss_url)\n",
        "\n",
        "            if feed.entries:\n",
        "                for entry in feed.entries:\n",
        "                    try:\n",
        "                        # Try to parse the published date from the entry\n",
        "                        pubdate = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %Z')\n",
        "                    except (AttributeError, ValueError):\n",
        "                        logging.warning(f\"Failed to parse date for article: {entry.title}\")\n",
        "                        continue\n",
        "\n",
        "                    # Check if the article's publication date falls within the specified range\n",
        "                    if self.start_date <= pubdate <= self.end_date:\n",
        "                        title = entry.title\n",
        "                        link = entry.link\n",
        "                        description = entry.summary if hasattr(entry, 'summary') else entry.description\n",
        "                        source = entry.source.title if hasattr(entry, 'source') and hasattr(entry.source, 'title') else 'Unknown'\n",
        "                        articles.append({\n",
        "                            'Title': title,\n",
        "                            'Link': link,\n",
        "                            'Description': description,\n",
        "                            'Published': pubdate,\n",
        "                            'Source': source\n",
        "                        })\n",
        "            else:\n",
        "                logging.info(f\"No articles found for date: {current_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            current_date += timedelta(days=1)\n",
        "\n",
        "        return articles\n",
        "\n",
        "# Function to fetch articles for multiple queries\n",
        "def fetch_articles(queries, start_date, end_date, language, region=\"US\"):\n",
        "    all_articles = []\n",
        "\n",
        "    for query in queries:\n",
        "        logging.info(f\"Fetching news for query: {query.strip()}\")\n",
        "        scraper = GoogleNewsFeedScraper(query.strip(), start_date, end_date, language, region)\n",
        "        articles = scraper.scrape_google_news_feed()\n",
        "        all_articles.extend(articles)\n",
        "        logging.info(f\"Fetched {len(articles)} articles for query: {query.strip()}\")\n",
        "        logging.info(\"=\"*80)\n",
        "\n",
        "    return all_articles\n",
        "\n",
        "# User inputs\n",
        "keywords = input(\"Enter keywords (comma separated for multiple): \").split(',')\n",
        "start_date_str = input(\"Enter start date (YYYY-MM-DD): \")\n",
        "end_date_str = input(\"Enter end date (YYYY-MM-DD): \")\n",
        "language = input(\"Enter language (e.g., en for English, ar for Arabic): \")\n",
        "region = input(\"Enter region code (e.g., US for the United States, EG for Egypt): \")\n",
        "\n",
        "# Convert the date strings to datetime objects\n",
        "try:\n",
        "    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n",
        "    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}. Please ensure dates are in the format YYYY-MM-DD.\")\n",
        "    raise\n",
        "\n",
        "# Fetch and display the news articles\n",
        "all_articles = fetch_articles(keywords, start_date, end_date, language, region)\n",
        "\n",
        "# Convert the list of articles to a DataFrame and remove duplicates\n",
        "df = pd.DataFrame(all_articles).drop_duplicates(subset=['Title', 'Link'])\n",
        "\n",
        "# Save to CSV for review\n",
        "output_file = \"google_news_articles.csv\"\n",
        "df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
        "print(f\"Articles saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "4Gj_TvX2vYzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "OL3c0xUn0CZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv('/content/google_news_articles (2).csv')"
      ],
      "metadata": {
        "id": "99Qc1zO0yXi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning 'Description' column by removing HTML tags\n",
        "df['Description'] = df['Description'].str.replace(r'<.*?>', '', regex=True)"
      ],
      "metadata": {
        "id": "3NBEHevDuA4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googlenewsdecoder import new_decoderv1\n",
        "\n",
        "# Function to decode Google News URLs\n",
        "def decode_google_news_url(url):\n",
        "    try:\n",
        "        decoded = new_decoderv1(url)\n",
        "        if decoded.get(\"status\"):\n",
        "            return decoded[\"decoded_url\"]\n",
        "        else:\n",
        "            return f\"Error: {decoded['message']}\"\n",
        "    except Exception as e:\n",
        "        return f\"Exception occurred: {e}\"\n",
        "\n",
        "# Apply decoding function to 'Link' column and create a new 'Original Link' column\n",
        "df['Original Link'] = df['Link'].apply(decode_google_news_url)"
      ],
      "metadata": {
        "id": "_gEA_6CMuGzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('ksamarket28to1decen.xlsx', index=False)  # Save the dataframe as an Excel file named 'data.xlsx' and exclude the index column"
      ],
      "metadata": {
        "id": "eqMtOby4o7yu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyP1G6ZOflUx5/EiRovWbvvy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}